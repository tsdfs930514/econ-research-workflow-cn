---
description: "Pre-analysis identification strategy challenger with threat assessment"
user_invocable: true
---

# /devils-advocate — Identification Strategy Challenger

Systematic pre-analysis challenge to your identification strategy and research design. This skill produces a **threat assessment**, not code fixes. It is conceptual and should be run before or during analysis design.

**Scope distinction**: `/devils-advocate` is **pre-analysis** conceptual challenge (threats, alternative explanations, falsification tests). `econometrics-critic` is **post-analysis** code review (diagnostics, implementation, reporting).

## Activation

When the user runs `/devils-advocate`, read the current research design from:
1. `vN/docs/research_proposal.md` (if generated by `/interview-me`), or
2. Ask the user to describe: research question, identification strategy, key variables, and method.

---

## Assessment Structure

### 1. Universal Threats

Evaluate each threat for the specific research design. Not all apply — skip those that are clearly irrelevant.

| Threat | Question |
|--------|----------|
| **Omitted Variable Bias** | What unobserved factors correlate with both treatment and outcome? |
| **Reverse Causality** | Could Y be causing X rather than X causing Y? |
| **Measurement Error** | Is the treatment/outcome measured with error? What kind (classical, non-classical)? |
| **Selection Bias** | Are treated units systematically different from controls in unobservable ways? |
| **SUTVA Violation** | Could treatment of one unit spill over to affect other units? |
| **Functional Form** | Is the linear specification appropriate, or could the effect be nonlinear? |
| **External Validity** | Does the estimated effect generalize beyond the specific sample? |
| **Publication Bias / p-hacking** | Are you testing many specifications and reporting the best one? |

### 2. Method-Specific Threats

Based on the identified method, assess:

**DID / TWFE / Staggered DID**:
- Parallel trends: What pre-treatment evidence exists? What if trends diverge?
- Anticipation effects: Could units change behavior before treatment?
- Negative weights: With staggered timing, are some ATTs entering with wrong sign (de Chaisemartin & D'Haultfoeuille)?
- Treatment heterogeneity: Does the treatment effect vary across cohorts or time?
- Composition changes: Does the sample change over time (entry/exit)?

**IV / 2SLS**:
- Exclusion restriction: What channels could the instrument affect Y other than through X?
- Monotonicity: Could the instrument affect some units in the opposite direction?
- LATE vs ATE: Is the local average treatment effect policy-relevant?
- Weak instruments: What is the likely first-stage F? Is it borderline?
- Many instruments: If using multiple instruments, is there an overidentification concern?

**RDD**:
- Manipulation: Can agents sort around the cutoff? (McCrary density test)
- Bandwidth sensitivity: How much do results change with bandwidth?
- Placebo cutoffs: Are there effects at non-cutoff thresholds?
- Covariate balance: Are covariates smooth at the cutoff?
- Donut hole: Is there bunching exactly at the cutoff requiring a donut-hole design?

**Panel FE**:
- Time-varying confounders: What unobserved shocks coincide with treatment?
- Nickell bias: With short T and lagged dependent variable, is dynamic panel bias a concern?
- Clustering: Is the clustering level appropriate? Too few clusters?
- Serial correlation: Are errors autocorrelated within panels?

**SDID / Synthetic Control**:
- Donor quality: Does the synthetic control actually fit the treated unit pre-treatment?
- Weight concentration: Are weights spread across donors or concentrated on one?
- Interpolation vs extrapolation: Is the synthetic control interpolating within the donor pool?
- Placebo units: Do in-space placebo tests show the treated unit is unusual?
- Number of treated units: Is N too small for inference?

### 3. Alternative Explanations

For each key result the user expects, generate **3 alternative explanations** that could produce the same empirical pattern without the proposed causal mechanism:

```
Expected result: X has a positive effect on Y

Alternative 1: [description — e.g., common shock, confounding policy]
Alternative 2: [description — e.g., reverse causality channel]
Alternative 3: [description — e.g., selection/survivorship]
```

For each alternative, indicate how to **distinguish** it from the proposed explanation (different testable prediction, data pattern, timing).

### 4. Falsification Tests

Recommend specific falsification tests:

| Test Type | Description | What It Tests |
|-----------|-------------|---------------|
| **Placebo treatment** | Apply "treatment" at a fake time/event | Whether effect is specific to real treatment |
| **Placebo outcome** | Use an outcome that should NOT be affected | Whether something broader is driving results |
| **Placebo cutoff** (RDD) | Test at non-treatment cutoff values | Whether the discontinuity is unique to the real cutoff |
| **Subsample analysis** | Run on subgroups where effect should be stronger/weaker | Whether heterogeneity matches the theory |
| **Dose-response** | Test if larger treatment intensity gives larger effects | Whether the mechanism is the proposed one |
| **Bounding** | Oster (2019) or Altonji et al. bounds on OVB | How large must omitted variable bias be to explain away the result |
| **Pre-trend test** (DID) | Event study with pre-treatment coefficients | Whether trends were parallel before treatment |
| **Permutation / randomization inference** | Randomly reassign treatment and compare | Whether the observed effect is unusual under the null |

### 5. Threat Matrix

Summarize all findings in a threat matrix. Severity levels match `econometrics-critic` for consistency:

| # | Threat | Severity | Mitigation Available? | Recommended Action |
|---|--------|----------|-----------------------|--------------------|
| 1 | ... | Critical | Yes/No/Partial | ... |
| 2 | ... | High | ... | ... |
| 3 | ... | Medium | ... | ... |
| ... | ... | Low | ... | ... |

**Severity scale**:
- **Critical**: Threatens identification validity — must be addressed before analysis
- **High**: Seriously weakens causal claims — should be addressed with robustness test or design change
- **Medium**: Suboptimal but not fatal — note as limitation or run secondary check
- **Low**: Enhancement suggestion — address if feasible

---

## Output

Display the full threat assessment in the conversation. If a research proposal exists at `vN/docs/research_proposal.md`, suggest updating its "Threats" section with the top findings.

After the assessment, display:
```
Threat assessment complete.

Critical: N  |  High: N  |  Medium: N  |  Low: N

Suggested next steps:
  Address Critical/High threats before running main estimation.
  /interview-me     — Revise research design if needed
  /run-{method}     — Proceed to estimation (after addressing threats)
  /robustness       — Design robustness checks targeting identified threats
```

## Notes

- Be genuinely adversarial — the purpose is to find weaknesses, not to validate the design.
- A good research design survives this challenge. A weak one is improved by it.
- This skill does not generate or modify code files.
- If the user's design has no critical threats, say so — don't manufacture problems.

---
description: "Auto-detect methods from a research plan and orchestrate the full skill pipeline end-to-end"
user_invocable: true
---

# /run-pipeline — Automated Pipeline Orchestration

When the user invokes `/run-pipeline`, read a research plan, detect the econometric method(s), generate an ordered skill execution plan, and run it end-to-end.

## Flags

- `/run-pipeline` — interactive mode (confirm plan before execution)
- `/run-pipeline --quick` — skip confirmation, execute all steps automatically
- `/run-pipeline --replication <paper.pdf>` — replication mode: extract methods and variables from a published paper PDF

## Step 0: Collect Input

Determine the research plan source (check in this order):

1. **User-provided text** — if the user passes a description directly (e.g., `/run-pipeline DID analysis of minimum wage on employment`)
2. **Research proposal** — check for `docs/research_proposal.md` (generated by `/interview-me`)
3. **Paper PDF** — if `--replication <paper.pdf>` flag is used, read the PDF and extract:
   - Research question and hypothesis
   - Identification strategy and econometric method
   - Variable definitions (dependent, treatment, controls, instruments)
   - Data description
4. **Interactive** — if none of the above, ask the user for:
   - Research question (one sentence)
   - Identification strategy / econometric method
   - Dataset path
   - Key variables (dependent, treatment, controls)

From the input, extract:
- **Research question**: one-sentence summary
- **Causal method(s)**: primary method + any secondary/robustness methods
- **Variable definitions**: Y, D/T, X controls, Z instruments (if IV), running variable (if RDD)
- **Data path**: location of the dataset
- **Target journal**: Chinese core / English TOP5 / working paper (affects table formatting)

## Step 1: Method Detection & Skill Mapping

Parse the research plan to identify the econometric method(s) and map to `/run-*` skills:

| Detected Method | Mapped Skill |
|----------------|-------------|
| DID / TWFE / event study / staggered treatment / parallel trends | `/run-did` |
| IV / 2SLS / instrumental variable / exclusion restriction | `/run-iv` |
| RDD / regression discontinuity / cutoff / threshold / bandwidth | `/run-rdd` |
| Panel FE / RE / GMM / Hausman / within estimator | `/run-panel` |
| Synthetic DID / SDID / synthetic control + DID | `/run-sdid` |
| Bootstrap / wild cluster bootstrap / resampling | `/run-bootstrap` |
| Placebo test / randomization inference / permutation | `/run-placebo` |
| Logit / probit / propensity score / treatment effects / IPW | `/run-logit-probit` |
| LASSO / regularization / variable selection / post-double-selection | `/run-lasso` |

**Multi-method detection**: A plan may specify multiple methods (e.g., "main analysis: DID; robustness: IV as alternative"). Detect all and order them:
1. Primary method first
2. Alternative/robustness methods after

**Keyword matching**: Look for these patterns in the text:
- Explicit method names (case-insensitive): "difference-in-differences", "instrumental variable", "regression discontinuity", etc.
- Method abbreviations: "DID", "DiD", "IV", "2SLS", "RDD", "FE", "RE", "GMM", "SDID"
- Identifying assumptions: "parallel trends" → DID, "exclusion restriction" → IV, "manipulation test" → RDD
- Stata commands mentioned: `csdid` → DID, `ivreghdfe` → IV, `rdrobust` → RDD

## Step 2: Generate Execution Plan

Based on the detected method(s), generate an ordered skill sequence. The template is:

```
Pipeline Plan for: [Research Question]
Method: [Primary Method] (+ [Secondary Methods])
Data: [Dataset Path]

Execution Sequence:
  1. /data-describe          — Data exploration and descriptive statistics
  2. /run-{primary method}   — Main analysis
  3. /cross-check            — Cross-validate Stata vs Python
  4. /robustness             — Comprehensive robustness tests
  [5. /run-{secondary method} — Alternative estimation (if applicable)]
  [6. /run-placebo           — Placebo tests (if applicable)]
  7. /make-table             — Generate publication-quality tables
  8. /adversarial-review     — Adversarial quality assurance
  9. /score                  — Quality scoring (6 dimensions, 100 pts)
  10. /synthesis-report      — Comprehensive synthesis report

Expected outputs:
  - X .do files in code/stata/
  - X .py files in code/python/
  - X .tex tables in output/tables/
  - X figures in output/figures/
  - 1 synthesis report in docs/
```

### Plan Customization Rules

- If the method is DID with staggered treatment: include `/run-placebo` (timing placebo)
- If the method is IV: include `/run-placebo` (instrument placebo) after robustness
- If the method is RDD: skip `/run-placebo` (density test and placebo cutoffs are built into `/run-rdd`)
- If the user mentioned bootstrap inference: include `/run-bootstrap` after the main analysis
- If the plan mentions propensity score matching: include `/run-logit-probit` before the main DID/IV
- If the plan mentions variable selection: include `/run-lasso` before the main analysis

### User Confirmation

Unless `--quick` flag is set, present the plan and ask:

```
Proceed with this pipeline? [yes / edit / cancel]
- yes:    Execute all steps in order
- edit:   Modify the sequence (add, remove, or reorder steps)
- cancel: Abort pipeline
```

If the user chooses `edit`, allow them to modify the sequence and re-confirm.

## Step 3: Sequential Execution

Execute each skill in the plan sequentially:

### For each step:

1. **Announce**: Display which skill is about to run and its purpose
   ```
   [Step 2/10] Running /run-did — Main DID analysis...
   ```

2. **Execute**: Invoke the skill with appropriate parameters derived from Step 0
   - Pass the variable definitions, data path, and method-specific parameters
   - For `/run-*` skills: provide the regression specification
   - For `/cross-check`: point to the Stata log just generated
   - For `/make-table`: point to the stored estimates
   - For `/robustness`: provide the baseline specification from the main analysis

3. **Verify output**: After each skill completes, check:
   - Did the expected output files get created?
   - Are there any `r(xxx)` errors in Stata logs?
   - Is the output non-empty?

4. **Handle errors**:
   - If a Stata `r(xxx)` error occurs: **pause execution**, display the error, and ask the user:
     ```
     Error in Step 2 (/run-did): Stata r(111) — variable not found
     Options: [fix and retry / skip this step / abort pipeline]
     ```
   - If `--quick` mode and error occurs: still pause (errors should never be silently skipped)
   - If a non-critical warning occurs (e.g., convergence warning): log it and continue

5. **Log progress**: After each step, display:
   ```
   [Step 2/10] /run-did — COMPLETE
     Outputs: code/stata/03_reg_main.do, output/logs/reg_main.log, output/tables/tab_main_results.tex
     Status: Clean (no errors)
   ```

### Automatic /synthesis-report

After all planned skills complete (or after `/score` if it's the last analytical step), automatically invoke `/synthesis-report` to generate the final comprehensive report.

## Step 4: Pipeline Summary

After all steps complete, display a summary:

```
Pipeline Complete
=================
Research Question: [question]
Method: [method]

Step Results:
  1. /data-describe        — COMPLETE (2 files generated)
  2. /run-did              — COMPLETE (3 files generated)
  3. /cross-check          — COMPLETE (PASS — max diff 0.003%)
  4. /robustness           — COMPLETE (12/14 specs significant)
  5. /make-table           — COMPLETE (3 tables generated)
  6. /adversarial-review   — COMPLETE (avg score: 88/100)
  7. /score                — COMPLETE (82/100 — Major Revisions)
  8. /synthesis-report     — COMPLETE (docs/ANALYSIS_SUMMARY.md)

Final Quality Score: 82/100
Status: Major Revisions — consider running /adversarial-review again after fixes

Output Files:
  code/stata/     — 5 .do files
  code/python/    — 2 .py files
  output/tables/  — 3 .tex tables
  output/figures/ — 2 .pdf figures
  output/logs/    — 5 .log files
  docs/           — ANALYSIS_SUMMARY.md, ANALYSIS_SUMMARY.tex, QUALITY_SCORE.md
```

### Score-Based Recommendations

| Final Score | Recommendation |
|-------------|---------------|
| >= 95 | "Publication ready. Consider running `/compile-latex` and `/commit`." |
| >= 90 | "Minor revisions needed. Review remaining issues in ANALYSIS_SUMMARY.md, fix, and re-run `/score`." |
| >= 80 | "Major revisions needed. Run `/adversarial-review` to identify specific fixes, then re-run affected `/run-*` skills." |
| < 80 | "Significant issues found. Review the synthesis report, address critical findings, and consider re-running the pipeline." |

## Replication Mode (--replication)

When `--replication <paper.pdf>` is specified:

1. Read the paper PDF using the PDF tool
2. Extract from the methodology section:
   - Econometric method(s) used
   - Regression specifications (equation form)
   - Variable names and definitions
   - Data source and sample restrictions
   - Fixed effects and clustering choices
   - Key robustness checks mentioned
3. Generate the execution plan as in Step 2, but with:
   - Variable names matching the original paper
   - Same specifications and diagnostics
   - Cross-validation as an additional step (not in the original paper but required by our workflow)
4. Proceed with execution as normal

The goal is to reproduce the paper's results and verify them through our quality infrastructure.
